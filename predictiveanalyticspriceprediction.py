# -*- coding: utf-8 -*-
"""PredictiveAnalyticsPricePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FRMsv0KskX0wSWbPLCXG4gJOgq-9M0cv

**Nama: Cindy Deviana Atmakusuma**

Proyek ini menggunakan dataset dari Kaggle untuk melakukan prediksi harga rumah (apartemen) di Moscow.

Link: https://www.kaggle.com/datasets/egorkainov/moscow-housing-price-dataset

# **Import Library yang digunakan**
"""

# Install public API Kaggle
!pip install -q kaggle

# Commented out IPython magic to ensure Python compatibility.
# Install library untuk proses data loading dan visualisasi data
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

# Impor library untuk data preparation
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Impor library untuk split data
from sklearn.model_selection import train_test_split

# Impor library GridSearchCV
from sklearn.model_selection import GridSearchCV

# Impor metrik
from sklearn.metrics import mean_squared_error

# Impor model
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor

"""# **Data Understanding**

Tahap ini dilakukan dalam rangka memahami informasi yang terdapat pada data dan juga berguna untuk menentukan kualitas dari data yang diperoleh.

**1. Data Loading**

Disini dataset yang digunakan akan dimuat agar dapat dipahami.
"""

# Membuat direktori baru bernama kaggle
!rm -rf ~/.kaggle && mkdir ~/.kaggle/

# Menyalin berkas kaggle.json pada direktori aktif saat ini ke direktori kaggle
!mv kaggle.json ~/.kaggle/kaggle.json

# Mengubah permission berkas
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset
!kaggle datasets download -d egorkainov/moscow-housing-price-dataset

# Ekstrak berkas zip
!unzip /content/moscow-housing-price-dataset.zip

houses = pd.read_csv('/content/data.csv')
houses

"""**2. Exploratory Data Analysis (EDA)**

Exploratory data analysis dilakukan untuk melakukan investigasi awal pada data untuk menganalisis karakteristik, menemukan pola, anomali, dan memeriksa asumsi pada data.

**2.1 Deskripsi Variabel**
"""

houses.info()

# Fitur 'Metro station' tidak memengaruhi harga apartemen, maka akan dihilangkan
houses.drop(['Metro station'], axis=1, inplace=True)

houses.describe()

"""**2.2 Menangani Missing Value dan Outliers**"""

min_to_metro = (houses['Minutes to metro'] == 0).sum()
num_of_rooms = (houses['Number of rooms'] == 0).sum()

print("Nilai 0 di kolom 'Minutes to metro' ada: ", min_to_metro)
print("Nilai 0 di kolom 'Number of rooms' ada: ", num_of_rooms)

houses = houses[houses['Minutes to metro']!=0]
houses = houses[houses['Number of rooms']!=0]

houses.shape

houses.duplicated().sum()

houses.drop_duplicates()

houses.isna().sum()

data = houses.select_dtypes(exclude=['object'])
for column in data:
  plt.figure()
  sns.boxplot(data=data, x=column)

Q1 = houses.quantile(0.25)
Q3 = houses.quantile(0.75)
IQR = Q3-Q1
houses=houses[~((houses<(Q1-1.5*IQR))|(houses>(Q3+1.5*IQR))).any(axis=1)]

houses.shape

"""# **Univariate Analysis**

Univariate Analysis merupakan proses analisis data dengan membagi fitur pada dataset menjadi dua bagian, yaitu numerical features dan categorical features.
"""

categorical_features = ['Apartment type','Region','Renovation']
numerical_features = ['Price','Minutes to metro','Number of rooms','Area','Living area','Kitchen area','Floor','Number of floors']

"""**1. Categorical Features**"""

feature = categorical_features[0]
count = houses[feature].value_counts()
percent = 100*houses[feature].value_counts(normalize=True)
df = pd.DataFrame({'Jumlah sampel':count, 'Persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

feature = categorical_features[1]
count = houses[feature].value_counts()
percent = 100*houses[feature].value_counts(normalize=True)
df = pd.DataFrame({'Jumlah sampel':count, 'Persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

feature = categorical_features[2]
count = houses[feature].value_counts()
percent = 100*houses[feature].value_counts(normalize=True)
df = pd.DataFrame({'Jumlah sampel':count, 'Persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""**2. Numerical Features**"""

houses.hist(bins=50, figsize=(20,15))
plt.show()

"""# **Multivariate Analysis**

Multivariate Analysis merupakan jenis visualisasi data untuk menggambarkan informasi yang diperoleh pada lebih dari dua variabel. Visualisasi jenis ini digunakan untuk merepresentasikan hubungan dan pola yang terdapat dalam multidimensional data.

**1. Categorical Features**
"""

cat_features = houses.select_dtypes(include='object').columns.to_list()

for col in cat_features:
    sns.catplot(x=col, y="Price", kind="bar", dodge=False, height=4, aspect=3, data=houses, hue=col, palette="Set3", legend=False)
    plt.title("Rata-rata 'price' Relatif terhadap - {}".format(col))

"""**2. Numerical Features**

Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
"""

sns.pairplot(houses, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = houses.corr().round(2)
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

houses.drop(['Minutes to metro','Kitchen area','Floor','Number of floors'], inplace=True, axis=1)
houses.head()

"""# **Data Preparation**

Data Preparation merupakan tahapan penting dalam proses pengembangan model machine learning karena ini merupakan tahapan dimana dilakukan proses transformasi data menjadi bentuk yang cocok untuk proses pemodelan.

**1. Encoding**

Dalam melakukan proses encoding fitur kategori, salah satu teknik yang umum dilakukan adalah teknik one-hot-encoding.
"""

houses = pd.concat([houses, pd.get_dummies(houses['Apartment type'], prefix='Apartment type')],axis=1)
houses = pd.concat([houses, pd.get_dummies(houses['Region'], prefix='Region')],axis=1)
houses = pd.concat([houses, pd.get_dummies(houses['Renovation'], prefix='Renovation')],axis=1)
houses.drop(['Apartment type','Region','Renovation'], axis=1, inplace=True)
houses.head()

"""**2. Reduksi Dimensi menggunakan PCA**

Teknik ini dilakukan untuk mengurangi jumlah fitur dengan tetap mempertahankan informasi pada data. Teknik ini adalah teknik untuk mereduksi dimensi, mengekstraksi fitur, dan mentransformasi data dari "n-dimensional space" ke dalam sistem berkoordinat baru dengan dimensi m, dimana m lebih kecil dari n.
"""

sns.pairplot(houses[['Number of rooms','Area','Living area']], plot_kws={"s": 3});

sns.pairplot(houses[['Area','Living area']], plot_kws={"s": 2});

pca = PCA(n_components=2, random_state=42)
pca.fit(houses[['Area','Living area']])
princ_comp = pca.transform(houses[['Area','Living area']])

pca.explained_variance_ratio_.round(3)

pca = PCA(n_components=1, random_state=42)
pca.fit(houses[['Area','Living area']])
houses['Areas'] = pca.transform(houses.loc[:, ('Area','Living area')]).flatten()
houses.drop(['Area','Living area'], axis=1, inplace=True)
houses.head()

"""**3. Train-Test-Split**

Membagi dataset menjadi data latih (train) dan data uji (test) sebelum membuat model. Pada kasus ini, dilakukan pembagian dataset train dan test dengan proporsi 90:10.
"""

X = houses.drop(["Price"], axis=1)
y = houses["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""**4. Standarisasi**

Menggunakan StandarScaler dari library *scikit-learn*. StandardScaler melakukan proses standarisasi fitur dengan mengurangkan mean (nilai rata-rata) kemudian membaginya dengan standar deviasi untuk menggeser distribusi.  StandardScaler menghasilkan distribusi dengan standar deviasi sama dengan 1 dan mean sama dengan 0. Sekitar 68% dari nilai akan berada di antara -1 dan 1.
"""

numerical_features = ['Number of rooms','Areas']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

X_train[numerical_features].describe().round(4)

"""# **Model Development**

Pada tahap ini, akan dikembangkan model machine learning dengan empat algoritma, lalu akan dilakukan evaluasi performa model dengan masing-masing algoritma untuk menentukan algoritma mana yang memberi hasil prediksi terbaik. Model yang digunakan adalah sebagai berikut:


1.   Support Vector Regression (SVR)
2.   Decision Trees
3.   K-Nearest Neighbor
4.   Random Forest


"""

models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['SVR', 'Decision Trees', 'KNN', 'Random Forest'])

# Grid Search Support Vector Regression
svr = SVR()
param_grid_svr = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['linear', 'rbf']}
grid_search_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring='neg_mean_squared_error')
grid_search_svr.fit(X_train, y_train)
print("Best parameters for SVR:", grid_search_svr.best_params_)

# SVR dengan parameter terbaik
svr_best = SVR(**grid_search_svr.best_params_)
svr_best.fit(X_train, y_train)
models.loc['train_mse','SVR'] = mean_squared_error(y_pred=svr_best.predict(X_train), y_true=y_train)

# Grid Search Decision Trees
dt = DecisionTreeRegressor()
param_grid_dt = {'max_depth': [None, 10, 20, 30, 40, 50], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10]}
grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='neg_mean_squared_error')
grid_search_dt.fit(X_train, y_train)
print("Best parameters for Decision Trees:", grid_search_dt.best_params_)

# Decision Trees dengan parameter terbaik
dt_best = DecisionTreeRegressor(**grid_search_dt.best_params_)
dt_best.fit(X_train, y_train)
models.loc['train_mse','Decision Trees'] = mean_squared_error(y_pred=dt_best.predict(X_train), y_true=y_train)

# Grid Search K-Nearest Neighbor
knn = KNeighborsRegressor()
param_grid_knn = {'n_neighbors': range(1, 21)}
grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='neg_mean_squared_error')
grid_search_knn.fit(X_train, y_train)
print("Best n_neighbors:", grid_search_knn.best_params_)

# K-Nearest Neighbor dengan parameter terbaik
knn_best = KNeighborsRegressor(**grid_search_knn.best_params_)
knn_best.fit(X_train, y_train)
models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn_best.predict(X_train), y_true=y_train)

# Grid Search Random Forest
param_grid_rf = {
  'n_estimators': [10, 50, 100, 200],
  'max_depth': [None, 10, 20, 30, 40, 50],
  'min_samples_leaf': [1, 2, 4],
  'min_samples_split': [2, 5, 10]
}
RF = RandomForestRegressor(random_state=42, n_jobs=-1)
grid_search_rf = GridSearchCV(RF, param_grid_rf, cv=5, scoring='neg_mean_squared_error')
grid_search_rf.fit(X_train, y_train)
print("Best parameters:", grid_search_rf.best_params_)

# Random Forest dengan parameter terbaik
RF_best = RandomForestRegressor(**grid_search_rf.best_params_)
RF_best.fit(X_train, y_train)
models.loc['train_mse','Random Forest'] = mean_squared_error(y_pred=RF_best.predict(X_train), y_true=y_train)

"""# **Evaluasi Model**

Metrik yang digunakan untuk evaluasi model adalah Mean Squared Error, dimana MSE menghitung selisih antara nilai prediksi model dan nilai sebenarnya dari data, kemudian mengkuadratkan selisih yang diperoleh agar tidak ada selisih yang memiliki nilai negatif. Lalu, selisih kuadrat kemudian dijumlahkan dan diambil sebagai rata-rata dari semua sampel data.
"""

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['SVR','Decision Trees', 'KNN', 'Random Forest'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'SVR': svr_best, 'Decision Trees': dt_best, 'KNN': knn_best, 'Random Forest': RF_best}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediksi = X_test.iloc[5:10].copy()
pred_dict = {'y_true':y_test[5:10]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)